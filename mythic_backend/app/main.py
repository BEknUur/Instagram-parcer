# app/main.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path

import asyncio
from pydantic import AnyUrl
import json, logging, datetime

from app.config import settings
from app.services.apify_client import run_actor, fetch_run, fetch_items, run_comment_scraper
from app.services.downloader import download_photos

log = logging.getLogger("api")
app = FastAPI(
    title="Mythic Instagram Parser API",
    description="–ï–¥–∏–Ω—ã–π endpoint: –ø–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ—Ñ–∏–ª—è Instagram (–ø–æ—Å—Ç—ã, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, stories, highlights) —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º JSON –∏ –∑–∞–≥—Ä—É–∑–∫–æ–π –º–µ–¥–∏–∞.",
    version="1.0.0"
)

# –î–æ–±–∞–≤–ª—è–µ–º CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5174",
        "http://localhost:3000",
        "http://104.248.18.254",
        "https://mythicai.me",
        "https://www.mythicai.me"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
def health_check():
    return {"status": "ok", "service": "instagram-parser", "version": "1.0.0"}


@app.get("/start-scrape")
async def start_scrape(
    url: AnyUrl,
    username: str
):
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ Instagram –ø—Ä–æ—Ñ–∏–ª—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–π JSON —Å—Ä–∞–∑—É"""
    clean_url = str(url).rstrip("/")
    
    user_identifier = f"user_{username.lower()}"

    run_input = {
        "directUrls":     [clean_url],
        "resultsType":    "posts",
        "resultsLimit": 50,
        "searchLimit": 1,              # –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø—Ä–æ—Ñ–∏–ª—å
        "searchType": "user",
    }

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–∫—Ç–æ—Ä –ë–ï–ó webhook - –±—É–¥–µ–º –∂–¥–∞—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
    run = await run_actor(run_input)
    run_id = run["id"]
    
    log.info(f"üöÄ –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–∞—á–∞—Ç –¥–ª—è {username}, runId={run_id}")
    
    # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∞–∫—Ç–æ—Ä–∞ (–º–∞–∫—Å–∏–º—É–º 10 –º–∏–Ω—É—Ç)
    max_wait_time = 600  # 10 –º–∏–Ω—É—Ç
    check_interval = 10  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥
    elapsed_time = 0
    
    while elapsed_time < max_wait_time:
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∞–∫—Ç–æ—Ä–∞
            run_status = await fetch_run(run_id)
            status = run_status.get("status")
            
            log.info(f"‚è≥ –°—Ç–∞—Ç—É—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ {run_id}: {status} (–ø—Ä–æ—à–ª–æ {elapsed_time}—Å)")
            
            if status == "SUCCEEDED":
                # –ê–∫—Ç–æ—Ä –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —É—Å–ø–µ—à–Ω–æ - –ø–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                dataset_id = run_status.get("defaultDatasetId")
                if not dataset_id:
                    raise HTTPException(500, "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å dataset_id")
                
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                items = await fetch_items(dataset_id, limit=run_input["resultsLimit"])
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω–æ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                run_dir = Path("data") / run_id
                run_dir.mkdir(parents=True, exist_ok=True)
                
                user_meta = {
                    "user_id": user_identifier,
                    "username": username,
                    "instagram_url": clean_url,
                    "created_at": datetime.datetime.now().isoformat(),
                    "sync_request": True
                }
                (run_dir / "user_meta.json").write_text(json.dumps(user_meta, ensure_ascii=False, indent=2), encoding="utf-8")
                (run_dir / "posts.json").write_text(json.dumps(items, ensure_ascii=False, indent=2), encoding="utf-8")
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ—Ç–∫–ª—é—á–µ–Ω–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
                # images_dir = run_dir / "images"
                # asyncio.create_task(download_photos_async(items, images_dir))
                
                log.info(f"‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è {username}. –ü–æ–ª—É—á–µ–Ω–æ {len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω—ã–π JSON
                return {
                    "success": True,
                    "runId": run_id,
                    "username": username,
                    "url": clean_url,
                    "message": f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω! –ü–æ–ª—É—á–µ–Ω–æ {len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö",
                    "data": items,  # üéØ –ü–û–õ–ù–´–ô JSON –û–¢–í–ï–¢
                    "stats": {
                        "total_items": len(items),
                        "profile_data": len([item for item in items if item.get("username")]),
                        "posts_with_comments": len([item for item in items if item.get("latestPosts") and any(post.get("commentsCount", 0) > 0 for post in item.get("latestPosts", []))]),
                        "processing_time_seconds": elapsed_time
                    }
                }
                
            elif status == "FAILED":
                raise HTTPException(500, f"–ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è: {run_status.get('statusMessage', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
            
            elif status in ["RUNNING", "READY"]:
                # –ê–∫—Ç–æ—Ä –µ—â–µ —Ä–∞–±–æ—Ç–∞–µ—Ç - –∂–¥–µ–º
                await asyncio.sleep(check_interval)
                elapsed_time += check_interval
            else:
                raise HTTPException(500, f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å –∞–∫—Ç–æ—Ä–∞: {status}")
                
        except Exception as e:
            if elapsed_time > 60:  # –ï—Å–ª–∏ –ø—Ä–æ—à–ª–æ –±–æ–ª—å—à–µ –º–∏–Ω—É—Ç—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫—É
                raise HTTPException(500, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}")
            else:
                # –ü–µ—Ä–≤—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –º–æ–≥—É—Ç –Ω–µ —Å—Ä–∞–±–æ—Ç–∞—Ç—å - –∂–¥–µ–º
                await asyncio.sleep(check_interval)
                elapsed_time += check_interval
    
    raise HTTPException(408, f"–ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –∑–∞ {max_wait_time} —Å–µ–∫—É–Ω–¥.")


@app.get("/scrape-comments")
async def scrape_comments(
    post_urls: str,
    results_limit: int = 100
):
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –ø–æ–¥ –ø–æ—Å—Ç–∞–º–∏ Instagram —á–µ—Ä–µ–∑ apify/instagram-comment-scraper
    
    Args:
        post_urls: URL –ø–æ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é
        results_limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
    """
    
    # –†–∞–∑–¥–µ–ª—è–µ–º URL
    urls_list = [url.strip() for url in post_urls.split(',') if url.strip()]
    
    if not urls_list:
        raise HTTPException(400, "–£–∫–∞–∂–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω URL –ø–æ—Å—Ç–∞")
    
    if len(urls_list) > 50:
        raise HTTPException(400, "–ú–∞–∫—Å–∏–º—É–º 50 –ø–æ—Å—Ç–æ–≤ –∑–∞ –∑–∞–ø—Ä–æ—Å")
    
    log.info(f"üöÄ –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è {len(urls_list)} –ø–æ—Å—Ç–æ–≤")
    
    run_input = {
        "directUrls": urls_list,
        "resultsLimit": results_limit,
    }
    
    try:
        run = await run_comment_scraper(run_input)
        run_id = run["id"]
        
        log.info(f"üîÑ Comment scraper –∑–∞–ø—É—â–µ–Ω, runId={run_id}")
        
        # –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è (–º–∞–∫—Å 5 –º–∏–Ω—É—Ç)
        max_wait_time = 300
        check_interval = 10
        elapsed_time = 0
        
        while elapsed_time < max_wait_time:
            run_status = await fetch_run(run_id)
            status = run_status.get("status")
            
            log.info(f"‚è≥ –°—Ç–∞—Ç—É—Å: {status} ({elapsed_time}—Å)")
            
            if status == "SUCCEEDED":
                dataset_id = run_status.get("defaultDatasetId")
                if not dataset_id:
                    raise HTTPException(500, "dataset_id –Ω–µ –ø–æ–ª—É—á–µ–Ω")
                
                comments = await fetch_items(dataset_id, limit=results_limit)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                save_dir = Path("data/comments") / run_id
                save_dir.mkdir(parents=True, exist_ok=True)
                (save_dir / "comments.json").write_text(
                    json.dumps(comments, ensure_ascii=False, indent=2),
                    encoding="utf-8"
                )
                
                log.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(comments)} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤")
                
                return {
                    "success": True,
                    "runId": run_id,
                    "message": f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(comments)} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤",
                    "total_comments": len(comments),
                    "posts_count": len(urls_list),
                    "processing_time_seconds": elapsed_time,
                    "comments": comments
                }
                
            elif status == "FAILED":
                raise HTTPException(500, f"–û—à–∏–±–∫–∞: {run_status.get('statusMessage')}")
            
            elif status in ["RUNNING", "READY"]:
                await asyncio.sleep(check_interval)
                elapsed_time += check_interval
            else:
                raise HTTPException(500, f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {status}")
        
        raise HTTPException(408, f"–¢–∞–π–º–∞—É—Ç {max_wait_time}—Å")
        
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {e}")
        raise HTTPException(500, str(e))


async def download_photos_async(items, images_dir):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π"""
    try:
        await download_photos(items, images_dir)
        log.info(f"üì∏ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ {images_dir}")
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
